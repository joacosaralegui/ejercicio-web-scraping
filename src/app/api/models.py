"""
Simple models to represent the relevant info provided and 
generated by the API and make any suitable validations.
"""

from datetime import datetime
from pydantic import BaseModel, ValidationError, validator
from typing import List, Optional
from fastapi import HTTPException

class CrawlLogSchema(BaseModel):
    """
    This information is whats gonna be stored in the database,
    this model must be in accordance with the crawl_logs Table of the database.
    """
    url: str
    amount_of_links: int
    date: datetime

class CrawlRequest(BaseModel):
    """
    Payload expected when a request is given to crawl a website and get the links
    """
    url: str
    
    @validator('url')
    def url_cant_be_empty(cls, url): 
        if not url or len(url) == 0:
            raise HTTPException(status_code=404, detail="Invalid URL. Please verify the parameter 'url' is set in the POST body.")

        return url

class CrawlResponse(CrawlRequest):
    """
    Response generated to the Crawl Request. Adds all relevant info
    """
    links: List[str]
    last_request_date: Optional[datetime]
    amount_difference: Optional[int]
    info: Optional[str]


